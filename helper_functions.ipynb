{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "author_graph = nx.read_gpickle(\"author_graph.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Occuring Keyword in Author's Publications:\n",
    "Param: author graph, name of author to be inspected, number of keywords to be returned\n",
    "\n",
    "Return: dictionary of most occuring keywords mapped to count of each keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_keywords(author_name, author_graph, return_count=20):\n",
    "    keywords = {}\n",
    "    publications = author_graph.nodes[author_name][\"Publications\"]\n",
    "    for publication in publications:\n",
    "        pub_list = (str)(publication).split()\n",
    "        for word in pub_list:\n",
    "            word = word.replace(\".\", \"\")\n",
    "            word = word.replace(\"?\", \"\")\n",
    "            if len(word) > 4:\n",
    "                if word in keywords and word[0].isupper():\n",
    "                    keywords.update({word : keywords[word] + 1})\n",
    "                else:\n",
    "                    keywords.update({word : 1})\n",
    "    keywords = sorted(keywords.items(), key=lambda item: item[1], reverse=True)\n",
    "    return keywords[0:return_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "<class 'networkx.classes.graph.Graph'>\n",
      "[('Based', 44), ('Network', 16), ('System', 14), ('Neural', 13), ('Method', 12), ('Learning', 12), ('Algorithm', 11), ('Networks', 11), ('Study', 11), ('Using', 11), ('Model', 11), ('Research', 10), ('Design', 10), ('Analysis', 9), ('Classification', 7), ('Detection', 7), ('Internet', 6), ('Segmentation', 6), ('Power', 6), ('Optimization', 5)]\n"
     ]
    }
   ],
   "source": [
    "print(len(author_graph.nodes[\"Wei Li\"][\"Publications\"]))\n",
    "print(type(author_graph))\n",
    "print(relevant_keywords(\"Wei Li\", author_graph, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Occuring Keyword in Coauthor's Publications:\n",
    "\n",
    "Finds most occuring keywords in publication titles of every coauthor of author\n",
    "\n",
    "Param: author graph, name of author to be inspected, number of keywords to be returned\n",
    "\n",
    "Return: dictionary of most occuring keywords mapped to count of each keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coauthor_relevant_keywords(author_name, author_graph, return_count=20):\n",
    "    keywords = {}\n",
    "    coauthor_list = list(nx.dfs_preorder_nodes(author_graph, source=author_name, depth_limit=1))\n",
    "    for coauthor in coauthor_list:\n",
    "        if coauthor == author_name:\n",
    "            continue\n",
    "        coauthor_keywords = relevant_keywords(coauthor)\n",
    "        for word in coauthor_keywords:\n",
    "            if word[0] in keywords:\n",
    "                keywords.update({word[0] : keywords[word[0]] + word[1]})\n",
    "            else:\n",
    "                keywords.update({word[0] : 1})\n",
    "    keywords = sorted(keywords.items(), key=lambda item: item[1], reverse=True)\n",
    "    return keywords[0:return_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(author_graph.nodes[\"Wei Li\"][\"Publications\"]))\n",
    "print(coauthor_relevant_keywords(\"Wei Li\", author_graph, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Relevant Phrases in Author Publication Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "def relevant_phrases(author_name, author_graph):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    author_publications = author_graph.nodes[author_name][\"Publications\"]\n",
    "    phrase_list = [] #list of relevant phrases to be returned\n",
    "    for pub in author_publications:\n",
    "        language = \"en\"\n",
    "        max_ngram_size = 3\n",
    "        deduplication_threshold = 0.9\n",
    "        numOfKeywords = 3\n",
    "        custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "        keywords = custom_kw_extractor.extract_keywords(pub)\n",
    "        for kw in keywords:\n",
    "            phrase_list.append(kw[0].lower())\n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_phrases(\"Radu Timofte\", author_graph)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8ad7cc9a111ac577fec804d99d295f3f6da60a5d648a58c5540db810996fd30"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
